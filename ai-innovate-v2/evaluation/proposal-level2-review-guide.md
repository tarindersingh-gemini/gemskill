# Stage 3 Level 2: Mentor Manual Review Guide

## Purpose
This guide helps mentors conduct thorough, consistent, and fair manual reviews of proposals that have passed Level 1 automated filtering. Mentors will evaluate Team Capability, validate automated scores, and make final approval decisions.

---

## Mentor Responsibilities

### Your Role
As a mentor reviewer, you are responsible for:
- **Completing the evaluation** that automation cannot do (Team Capability assessment)
- **Validating automated scores** and adjusting if the algorithm missed important context
- **Making final approval decisions** based on total scores and holistic assessment
- **Providing constructive feedback** to all teams, approved or declined
- **Capacity planning** to ensure approved teams can be properly supported
- **Maintaining fairness** and consistency across all reviews

### Time Commitment
- **Per proposal:** 15-20 minutes for thorough review
- **Expected volume:** 15-30 proposals (total 5-10 hours)
- **Deadline:** Complete reviews within 3 business days

---

## Review Process

### Step 1: Preparation (Before Reviewing)

**1.1 Calibration Exercise**
- Review 2-3 sample proposals with co-mentor
- Score independently then compare
- Discuss any significant scoring differences
- Align on interpretation of rubrics
- Establish consistent standards

**1.2 Understand Context**
- Review total number of submissions
- Understand resource capacity constraints
- Know target approval rate (40-60%)
- Review automated scoring distribution

**1.3 Set Up Review Environment**
- Block dedicated time for reviews
- Access all proposals in GitHub
- Have evaluation criteria document open
- Prepare feedback templates
- Ready scoring spreadsheet/tool

### Step 2: Individual Proposal Review (Per Submission)

**2.1 Initial Read (3-5 minutes)**
- Read entire proposal without scoring
- Get overall sense of the idea
- Note initial impressions and questions
- Flag anything exceptional or concerning

**2.2 Review Automated Scores (2-3 minutes)**
- Review Level 1 agent's scores and rationales
- Check if scores seem reasonable
- Note any scores that seem off
- Identify areas needing validation

**2.3 Score Team Capability (3-5 minutes)**
Use the [Team Capability Rubric](#team-capability-scoring-rubric) (see below)
- Review team member information
- Assess relevant experience
- Evaluate skill set coverage
- Check role distribution
- Consider commitment indicators
- Assign score (0-10 points)

**2.4 Validate/Adjust Automated Scores (3-5 minutes)**
For each automated criterion, ask:
- **Innovation (25 pts):** Did the agent understand the novelty?
- **Feasibility (20 pts):** Did the agent assess timeline correctly?
- **Business Value (20 pts):** Did the agent grasp the impact?
- **Resources (15 pts):** Did the agent identify all resource needs?
- **Clarity (10 pts):** Did the agent fairly assess completeness?

Make adjustments where automation missed context:
- **Adjust UP:** If agent undervalued human judgment aspects
- **Adjust DOWN:** If agent missed red flags or overscored
- **Document:** Note reason for any adjustment >3 points

**2.5 Calculate Final Score**
- Sum all criteria scores (including Team Capability)
- **Total possible: 100 points**
- Compare to approval threshold (â‰¥60 points)
- Check for any criterion below 6 points (weakness flag)

**2.6 Holistic Assessment (2-3 minutes)**
Beyond the numbers, consider:
- **Gut check:** Does this team have a real shot at success?
- **Mentoring need:** How much support will they require?
- **Strategic fit:** Does this align with organizational priorities?
- **Diversity:** Does approving this add variety to the portfolio?
- **Risk/reward:** Is this a reasonable bet?

**2.7 Document Decision & Feedback (3-5 minutes)**
- Make approval decision
- Write strengths (2-3 specific points)
- Write improvement areas (2-3 constructive suggestions)
- Note any special considerations
- Prepare feedback message

### Step 3: Batch Review & Final Decisions

**3.1 Review All Proposals Holistically**
- Rank all proposals by total score
- Identify natural approval tiers
- Consider resource capacity
- Look for gaps in problem domain coverage
- Check for team/skill diversity

**3.2 Apply Approval Threshold**
**Approve proposals that meet ALL of:**
- âœ… Total score â‰¥ 60 points
- âœ… No criterion scores < 6 points
- âœ… At least one strength area (criterion scored 17+)
- âœ… No critical blockers identified

**Waitlist proposals that:**
- Score 55-59 points (borderline)
- Have potential but minor concerns
- May fit if capacity increases

**Decline proposals that:**
- Score < 55 points
- Have critical weaknesses (criteria < 6)
- Have blocking resource/feasibility issues
- Don't meet minimum standards

**3.3 Capacity Check**
- Count approved proposals
- Ensure it doesn't exceed mentor capacity
- If too many: Apply tighter criteria (raise threshold to 65)
- If too few: Consider borderline proposals (lower threshold to 57)

**3.4 Final Review**
- Review approval list for balance and fairness
- Check for any obvious outliers or mistakes
- Ensure feedback is complete for all teams
- Prepare summary report

---

## Team Capability Scoring Rubric

### Criterion: Team Capability (10 points)
*This is the ONLY criterion that requires human judgment and cannot be automated.*

**Score 9-10 (Strong Team):**

**Experience Indicators:**
- âœ… At least one team member has AI/ML project experience
- âœ… Team collectively has built applications or prototypes before
- âœ… Experience mentioned with relevant technologies (Python, APIs, web dev)
- âœ… Track record of completing projects or hackathons

**Skills Coverage:**
- âœ… Complementary skill sets (e.g., developer + designer + domain expert)
- âœ… All key roles covered: technical lead, development, testing/QA
- âœ… Someone has UI/UX skills if needed
- âœ… Domain knowledge relevant to problem

**Team Structure:**
- âœ… Clear role distribution (each member has defined responsibilities)
- âœ… Team size is 3-4 members (good collaboration size)
- âœ… Primary contact is clearly identified
- âœ… Team appears balanced (no single person doing everything)

**Commitment Indicators:**
- âœ… All team members from same or related departments (easier coordination)
- âœ… Team members have worked together before
- âœ… Enthusiasm evident in writing
- âœ… Realistic time commitment acknowledged

**Decision:** This team has an excellent chance of success. Minimal mentoring needed.

---

**Score 7-8 (Capable Team):**

**Experience Indicators:**
- âœ“ Some relevant technical experience mentioned
- âœ“ May not have AI/ML experience but strong dev background
- âœ“ Demonstrated problem-solving ability
- âˆ¼ Project experience is implied but not detailed

**Skills Coverage:**
- âœ“ Key skills are covered
- âœ“ Some complementary skills
- âˆ¼ May have minor gaps (e.g., no dedicated designer)
- âœ“ Can learn what's needed

**Team Structure:**
- âœ“ Roles are mentioned but could be clearer
- âœ“ Team size is 2-4 members
- âœ“ Primary contact identified
- âˆ¼ Distribution seems reasonable

**Commitment Indicators:**
- âœ“ Team seems available and interested
- âˆ¼ May be from different departments (coordination needed)
- âœ“ Professional tone suggests seriousness
- âˆ¼ No specific concerns about commitment

**Decision:** This team can succeed with moderate mentoring and support.

---

**Score 5-6 (Developing Team):**

**Experience Indicators:**
- âˆ¼ Limited technical experience mentioned
- âš  No AI/ML experience evident
- âˆ¼ May be first-time hackathon participants
- âš  Project experience is unclear

**Skills Coverage:**
- âˆ¼ Some key skills present
- âš  Significant skill gaps
- âš  May lack critical expertise (e.g., no developer on UI-heavy project)
- âˆ¼ Steep learning curve expected

**Team Structure:**
- âš  Roles are vague or undefined
- âˆ¼ Team size is 1-2 members (collaboration limited)
- âœ“ Primary contact identified
- âš  Unclear who will do what

**Commitment Indicators:**
- âˆ¼ Commitment is not clearly stated
- âš  Team members from very different departments
- âˆ¼ Some enthusiasm but uncertain follow-through
- âš  Availability may be an issue

**Decision:** This team will need significant mentoring. Success is possible but less likely. Consider for approval only if proposal is exceptionally strong in other areas or if building capability is a priority.

---

**Score 3-4 (Weak Team):**

**Experience Indicators:**
- âŒ Minimal or no relevant experience
- âŒ No AI/ML background
- âŒ No project completion track record
- âŒ Appears to be very early in technical journey

**Skills Coverage:**
- âŒ Critical skill gaps
- âŒ No clear technical leader
- âŒ Missing essential expertise
- âŒ Learning curve is too steep for timeline

**Team Structure:**
- âŒ Solo or poorly defined team
- âŒ No clear role distribution
- âŒ Team structure doesn't match project needs
- âŒ Unclear collaboration plan

**Commitment Indicators:**
- âŒ Low confidence in commitment
- âŒ Major availability concerns
- âŒ Lack of enthusiasm evident
- âŒ Team structure suggests coordination problems

**Decision:** Success is highly unlikely. This team needs more preparation. Recommend declining with encouragement to build skills and try next time.

---

**Score 0-2 (Inadequate Team):**

**Red Flags:**
- âŒ No relevant experience at all
- âŒ Critical misunderstandings of technology
- âŒ Solo participant with unrealistic scope
- âŒ Team cannot execute even simplified version
- âŒ No availability during hackathon period
- âŒ Major skill gaps with no plan to address

**Decision:** Team is not ready for this hackathon. Decline with constructive feedback and resources for skill development.

---

## Validation Guidelines

### When to Adjust Automated Scores

**Adjust Innovation Score UP if:**
- Automation missed domain-specific novelty
- Proposal cleverly combines existing tools in unique way
- Problem-solution fit is more creative than agent recognized
- Industry context makes this more innovative than it appears

**Adjust Innovation Score DOWN if:**
- Proposal is actually a duplicate of existing tool
- "Innovation" is just adding AI to standard process
- Differentiation claims are exaggerated
- Novelty is superficial

**Adjust Feasibility Score UP if:**
- Team has proven track record agent couldn't assess
- Technologies are more accessible than agent knew
- Implementation plan is stronger than agent recognized
- Team has insider knowledge making it more feasible

**Adjust Feasibility Score DOWN if:**
- Agent missed critical technical blockers
- Timeline is even more unrealistic than scored
- Dependencies are harder to meet than agent assessed
- Technical approach has fundamental flaws

**Adjust Business Value Score UP if:**
- Impact is greater than described in proposal
- You have insider knowledge of problem severity
- Scalability potential is higher than agent assessed
- Strategic alignment is stronger than recognized

**Adjust Business Value Score DOWN if:**
- Problem is less severe than described
- User base is smaller than claimed
- Similar solution already exists and is adequate
- Benefits are theoretical rather than practical

**Adjust Resource Score UP if:**
- Requests are more reasonable than agent assessed
- Free alternatives exist that agent didn't know about
- Resources are more accessible than agent knew
- Budget is actually conservative

**Adjust Resource Score DOWN if:**
- Hidden costs not captured in estimate
- Resources are less available than claimed
- Agent missed resource blockers
- Budget is optimistic or incomplete

**Adjust Clarity Score UP if:**
- Proposal is clearer than automated assessment
- Technical jargon made it seem unclear to agent
- Context makes it clearer than it reads
- Agent penalized unnecessarily

**Adjust Clarity Score DOWN if:**
- Proposal is actually less clear than scored
- Important sections are genuinely vague
- Agent was too generous
- Critical details are missing

---

## Common Evaluation Pitfalls

### âŒ Avoid These Biases

**1. Novelty Bias**
- âŒ "This is so cool and cutting-edge!" â†’ But can they actually build it?
- âœ… Balance innovation with feasibility and impact

**2. Simplicity Bias**
- âŒ "This is too simple, not impressive enough" â†’ But will it help 100 people daily?
- âœ… Value practical impact over complexity

**3. Familiarity Bias**
- âŒ "I understand this domain well, so this must be good" â†’ Is it actually innovative?
- âŒ "I don't understand this domain, so it seems risky" â†’ May be valid in that context
- âœ… Evaluate fairly regardless of your domain expertise

**4. Team Reputation Bias**
- âŒ "I know this person, they're great" â†’ But is THIS proposal solid?
- âŒ "I don't know this team, seems risky" â†’ May be hidden talent
- âœ… Evaluate the proposal and evidence, not your personal knowledge

**5. Perfection Bias**
- âŒ "This has some weaknesses, so it's not good enough" â†’ Everything has weaknesses
- âœ… Look for potential and whether weaknesses are manageable

**6. Risk Aversion Bias**
- âŒ "This seems risky, let's not approve it" â†’ Some risk is acceptable
- âœ… Assess if risks are identified and have mitigation strategies

**7. Recency Bias**
- âŒ "The last few were weak, so I'm being harsh" â†’ Each proposal deserves fresh eyes
- âŒ "I just approved several strong ones, need to be pickier now" â†’ Maintain consistent standards
- âœ… Take breaks between batches to reset perspective

### âœ… Best Practices

**1. Read Holistically First**
- Understand the full proposal before scoring
- Let the idea sink in before judging
- Consider context and constraints

**2. Score Independently**
- Evaluate each criterion on its own merits
- Don't let one score influence others
- Use the rubric, not gut feeling alone

**3. Think Long-Term**
- Could this become a permanent tool?
- Will this team grow from the experience?
- Does this build organizational capability?

**4. Be Constructive**
- Even declined proposals deserve helpful feedback
- Identify specific improvement areas
- Encourage future participation

**5. Stay Calibrated**
- Compare notes with co-mentor regularly
- Discuss borderline cases
- Maintain scoring consistency

**6. Consider Portfolio Balance**
- Mix of innovative and practical projects
- Variety of problem domains
- Range of team experience levels (mostly strong, some developing)

---

## Special Situations

### Borderline Proposals (Score 55-59)
**Decision Framework:**
- âœ… **Approve if:** Team is strong (8+) and just one weakness area
- ðŸ” **Waitlist if:** Generally solid but resource constrained
- âŒ **Decline if:** Multiple moderate weaknesses with weak team

**Questions to Ask:**
- Could mentoring overcome the weaknesses?
- Is there untapped potential not captured in scores?
- Would approval add valuable diversity to approved portfolio?
- Do we have capacity to support a higher-need team?

### Highly Innovative but Risky Proposals
**Decision Framework:**
- âœ… **Approve if:** Team is very strong, risks are identified and mitigated
- ðŸ” **Waitlist if:** Need to see what other options exist first
- âŒ **Decline if:** Team is not capable of managing the risk

**Philosophy:** Some high-risk, high-reward projects can be worthwhile IF the team is exceptional. Aim for 10-20% of approved projects to be in this category.

### Practical but Uninspiring Proposals
**Decision Framework:**
- âœ… **Approve if:** High business value, clear impact, solid team
- âœ… **Approve if:** Fills important gap in problem domain coverage
- âŒ **Decline if:** Innovation score < 10 and no exceptional impact

**Philosophy:** "Boring but useful" can be valuable, especially if it demonstrates real ROI and could become a permanent tool.

### Solo Teams
**Decision Framework:**
- âš ï¸ **Caution:** Solo participants have higher failure rate
- âœ… **Approve if:** Scope is small, individual is highly capable, proposal is strong
- ðŸ” **Suggest:** Encourage finding team members before approval
- âŒ **Decline if:** Scope is ambitious or individual lacks track record

### Duplicate/Similar Proposals
**Decision Framework:**
- âœ… **Both approved:** If approaches are genuinely different
- âœ… **One approved:** Select stronger team or approach
- ðŸ” **Suggest team merger:** If teams are willing and compatible
- âŒ **Both declined:** If neither is strong enough on its own

### Resource-Intensive Proposals
**Decision Framework:**
- âœ… **Approve if:** Impact justifies cost and proposal is otherwise strong
- ðŸ” **Negotiate:** Can costs be reduced or free alternatives found?
- âŒ **Decline if:** Resources unavailable or costs unjustified

---

## Feedback Guidelines

### Feedback Principles
1. **Be specific:** Reference actual content from proposal
2. **Be constructive:** Frame weaknesses as opportunities
3. **Be encouraging:** Recognize effort and creativity
4. **Be actionable:** Provide clear suggestions for improvement
5. **Be respectful:** Maintain professional, positive tone

### Approval Feedback Template

```markdown
ðŸŽ‰ **Congratulations! Your proposal has been approved!**

Your team's proposal "[Problem Title]" has been selected to move forward!

**Final Evaluation Summary:**
- Innovation & Creativity: [Score]/25 [Adjusted from automated: X]
- Technical Feasibility: [Score]/20 [Adjusted from automated: X]
- Business Value & Impact: [Score]/20 [Adjusted from automated: X]
- Resource Reasonableness: [Score]/15 [Adjusted from automated: X]
- Team Capability: [Score]/10 [New]
- Clarity & Completeness: [Score]/10 [Automated]
- **Total Score: [Score]/100**

**What Stood Out:**
- [Specific strength 1 - be specific, reference proposal content]
- [Specific strength 2 - be specific]
- [Specific strength 3 - optional]

**Areas to Focus On During Development:**
- [Constructive guidance 1 - specific and actionable]
- [Constructive guidance 2 - specific and actionable]
- [Risk to monitor or skill to develop]

**Resources & Next Steps:**
1. Resource allocation team will contact you by [Date]
2. Join your team channel: #hackathon-team-[team-name]
3. Development period: [Start Date] - [End Date]
4. Midway check-in due: [Midway Date]
5. Final submission due: [Final Date]

**Your Assigned Mentor:** [Name] - [Email]
Feel free to reach out with questions or for guidance!

Looking forward to seeing your prototype! ðŸš€

---
*Reviewed by: [Mentor Name]*
```

### Decline Feedback Template

```markdown
**Thank you for your thoughtful submission**

Thank you for submitting "[Problem Title]" to the AI Innovate Hackathon. After careful evaluation, we've decided not to move forward with this proposal at this time.

**Evaluation Summary:**
- Innovation & Creativity: [Score]/25
- Technical Feasibility: [Score]/20
- Business Value & Impact: [Score]/20
- Resource Reasonableness: [Score]/15
- Team Capability: [Score]/10
- Clarity & Completeness: [Score]/10
- **Total Score: [Score]/100**

**What We Appreciated:**
- [Genuine positive 1 - be specific]
- [Genuine positive 2 - be specific]

**Areas That Limited Selection:**
- [Constructive feedback 1 - specific, not harsh]
- [Constructive feedback 2 - specific, not harsh]
- [If applicable: Resource constraints, high competition, etc.]

**Recommendations for Future Submissions:**
- [Specific actionable advice 1]
- [Specific actionable advice 2]
- [Encouragement or resource suggestion]

**Moving Forward:**
We received [X] proposals and could only approve [Y] due to resource constraints. Your interest in AI innovation is valuable! We encourage you to:
- Join our AI Innovation Community: [Link/Channel]
- Attend AI learning sessions and workshops
- Consider applying to our next hackathon in [Timeframe]
- Explore the winning projects for inspiration

Thank you for your creativity and participation! ðŸ™

---
*Reviewed by: [Mentor Name]*
```

### Waitlist Feedback Template

```markdown
**Your proposal is on our waitlist**

Thank you for your submission "[Problem Title]". Your proposal scored well and is on our ranked waitlist!

**Evaluation Summary:**
- Innovation & Creativity: [Score]/25
- Technical Feasibility: [Score]/20
- Business Value & Impact: [Score]/20
- Resource Reasonableness: [Score]/15
- Team Capability: [Score]/10
- Clarity & Completeness: [Score]/10
- **Total Score: [Score]/100**

**Why You're on the Waitlist:**
Your proposal met our quality standards, but we reached our resource capacity limit. We approved [X] proposals and placed [Y] strong submissions on the waitlist.

**What This Means:**
- If any approved teams withdraw, we'll invite waitlist teams in rank order
- We'll notify you by [Date] if a spot opens up
- Keep your team available and engaged

**Your Proposal's Strengths:**
- [Strength 1]
- [Strength 2]

**Small Improvements That Could Strengthen Future Submissions:**
- [Minor suggestion 1]
- [Minor suggestion 2]

**Next Steps:**
- No action needed from you right now
- Check email for potential approval updates
- If not selected, we encourage participation in future hackathons

Thank you for your excellent work! ðŸŒŸ

---
*Reviewed by: [Mentor Name]*
```

---

## Review Checklist

### Before Starting Reviews
- [ ] Completed calibration exercise with co-mentor
- [ ] Understand approval target and resource constraints
- [ ] Have all documents and tools ready
- [ ] Scheduled dedicated review time
- [ ] Reviewed evaluation criteria thoroughly

### For Each Proposal
- [ ] Read entire proposal without scoring first
- [ ] Reviewed automated scores and rationale
- [ ] Scored Team Capability (0-10)
- [ ] Validated/adjusted automated scores if needed
- [ ] Documented any score adjustments >3 points
- [ ] Calculated final total score
- [ ] Made approval decision
- [ ] Wrote specific strengths (2-3)
- [ ] Wrote constructive feedback (2-3)
- [ ] Prepared feedback message

### After All Reviews
- [ ] Ranked all proposals by score
- [ ] Applied approval thresholds
- [ ] Checked capacity constraints
- [ ] Ensured portfolio balance
- [ ] Reviewed approval list for fairness
- [ ] Completed all feedback messages
- [ ] Prepared summary report
- [ ] Coordinated with co-mentor on any borderline decisions

### Before Finalizing
- [ ] Double-checked all decisions
- [ ] Ensured feedback is constructive and professional
- [ ] Verified no scoring or calculation errors
- [ ] Confirmed resource team is ready for approved teams
- [ ] Prepared for potential appeals or questions

---

## Post-Review Activities

### Summary Report
Create a brief report including:
- Total proposals reviewed
- Number approved, declined, waitlisted
- Score distribution
- Common strengths across proposals
- Common weaknesses across proposals
- Recommendations for future hackathons
- Time spent on reviews

### Team Handoff
For approved teams:
- Move proposals to `approved` directory
- Notify resource allocation team
- Set up team communication channels
- Send approval notifications
- Assign mentors for development phase

### Continuous Improvement
- Document challenging decisions
- Note any evaluation criteria ambiguities
- Suggest improvements to rubrics or process
- Share feedback with organizers
- Prepare for midway check-in phase

---

## FAQs for Mentors

**Q: What if I disagree significantly with automated scores?**
A: You have full authority to adjust scores. Document your reasoning for adjustments >3 points. The automated scores are guidance, not requirements.

**Q: Can I approve a proposal that scores below 60?**
A: Yes, if you believe there are exceptional circumstances or untapped potential. Document your reasoning thoroughly.

**Q: What if two proposals are nearly identical?**
A: Consider suggesting a team merger if compatible. Otherwise, approve the stronger team/approach or both if genuinely different.

**Q: How much weight should Team Capability have?**
A: Significant weight. Even a great idea will fail without a capable team. Strong teams (8-10 points) can overcome moderate weaknesses in other areas.

**Q: What if I'm unsure about technical feasibility?**
A: Consult with your co-mentor or a technical expert. Don't approve if you have serious feasibility doubts.

**Q: Should I be lenient or strict?**
A: Be fair and consistent. Use the rubrics objectively. Remember that approval means committing resources, so maintain standards.

**Q: How do I handle borderline cases?**
A: Use the holistic assessment factors: team strength, mentoring need, strategic fit, diversity. When in doubt, discuss with co-mentor.

**Q: What if we don't have enough strong proposals?**
A: Don't lower standards just to hit approval targets. It's better to approve fewer strong proposals than many weak ones.

---

**Remember:** Your goal is to select proposals with the highest likelihood of success that will provide valuable learning experiences and potentially lasting organizational value. Be thorough, fair, and constructive in all evaluations.
