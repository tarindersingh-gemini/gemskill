# Stage 6 Level 2: Mentor Evaluation - Shortlist Top 10

## Purpose
Mentors review all final submissions, validate automated scores, complete manual scoring components, and shortlist the top 10 submissions for the final judges panel.

## Overview

**Goal:** Select the 10 best submissions to advance to judges
**Timeline:** 2-3 days after final submission deadline
**Mentor Load:** ~15-30 submissions to review
**Time per submission:** 30-45 minutes

---

## Complete Evaluation Criteria (100 Points)

| Criterion | Weight | Points | Primary Evaluator |
|-----------|--------|--------|-------------------|
| Technical Execution & Quality | 25% | 25 | Mentor |
| AI Integration & Innovation | 20% | 20 | Mentor |
| Completeness & Polish | 15% | 15 | Automated + Mentor |
| Impact & Value | 15% | 15 | Mentor |
| Documentation & Presentation | 15% | 15 | Automated + Mentor |
| Innovation & Uniqueness | 10% | 10 | Mentor |

---

## Mentor Evaluation Process

### Step 1: Review Automated Scoring (5-10 min)
- Read Level 1 automated scoring report
- Note automated scores and flags
- Identify areas requiring verification
- Review strengths and concerns identified

### Step 2: Watch Demo Video (2-4 min)
- Watch the full demo video
- Verify working prototype claims
- Assess presentation quality
- Note actual AI capabilities shown
- Evaluate user experience and polish

### Step 3: Test Prototype (5-10 min)
**If live demo available:**
- Access the live demo URL
- Test core features
- Try edge cases
- Assess responsiveness and reliability

**If local setup:**
- Attempt to run locally (if reasonable)
- Or rely on demo video + code review

**If video only:**
- Thorough video analysis
- Code repository review for verification

### Step 4: Review Code Repository (5-10 min)
- Browse code structure
- Check code quality (sample files)
- Verify AI integration implementation
- Review README and documentation
- Check commit history (optional - shows effort)

### Step 5: Complete Manual Scoring (10-15 min)
Score all criteria using detailed rubrics (see below)
- Complete manual components
- Validate/adjust automated scores
- Calculate total score
- Document rationale

### Step 6: Write Feedback (5 min)
- Identify top 3 strengths
- Note areas for improvement
- Provide constructive feedback

---

## Detailed Scoring Rubrics

### 1. Technical Execution & Quality (25 points)

**What to Evaluate:**
- Does the prototype actually work?
- Is the code quality reasonable?
- Are errors handled appropriately?
- Is the technical implementation sound?

**Scoring:**

**Excellent (21-25 points):**
- ‚úÖ Prototype works smoothly with no major bugs
- ‚úÖ Code is well-structured and readable
- ‚úÖ Error handling is comprehensive
- ‚úÖ Technical choices are appropriate
- ‚úÖ Performance is good
- ‚úÖ Edge cases are handled
- ‚úÖ Professional quality

**Good (16-20 points):**
- ‚úÖ Prototype works with minor bugs
- ‚úÖ Code structure is reasonable
- ‚úÖ Basic error handling present
- ‚úÖ Technical approach is sound
- ‚úÖ Performance is acceptable
- Some polish needed but functional

**Fair (11-15 points):**
- ‚ö†Ô∏è Prototype works but has notable bugs
- ‚ö†Ô∏è Code quality is acceptable but could improve
- ‚ö†Ô∏è Limited error handling
- ‚ö†Ô∏è Technical approach has some issues
- ‚ö†Ô∏è Performance concerns
- Functional but rough around edges

**Poor (6-10 points):**
- ‚ùå Prototype barely works or frequent failures
- ‚ùå Code quality is poor
- ‚ùå Minimal error handling
- ‚ùå Technical approach has significant flaws
- ‚ùå Performance issues
- More proof-of-concept than working prototype

**Inadequate (0-5 points):**
- ‚ùå Doesn't work or mostly non-functional
- ‚ùå Code is very poor quality
- ‚ùå No error handling
- ‚ùå Technical approach is fundamentally flawed

---

### 2. AI Integration & Innovation (20 points)

**What to Evaluate:**
- Is AI actually integrated and functional?
- How innovative is the AI application?
- Is AI appropriate for the problem?
- Does AI add real value?

**Scoring:**

**Exceptional (17-20 points):**
- ‚úÖ AI is core to the solution and works excellently
- ‚úÖ Novel or creative AI application
- ‚úÖ AI significantly enhances value
- ‚úÖ Appropriate AI technology selection
- ‚úÖ Quality AI outputs demonstrated
- ‚úÖ Innovative prompt engineering or implementation

**Strong (13-16 points):**
- ‚úÖ AI is integrated and works well
- ‚úÖ Good application of AI technology
- ‚úÖ AI adds clear value
- ‚úÖ Appropriate technology choice
- ‚úÖ Good quality outputs
- Solid AI implementation

**Good (9-12 points):**
- ‚úÖ AI is present and functional
- ‚ö†Ô∏è Standard AI application
- ‚ö†Ô∏è AI adds some value
- ‚ö†Ô∏è Technology choice is reasonable
- Adequate AI integration

**Fair (5-8 points):**
- ‚ö†Ô∏è AI is present but limited
- ‚ö†Ô∏è Basic or superficial application
- ‚ö†Ô∏è Questionable AI necessity
- ‚ö†Ô∏è AI value is unclear
- Minimal innovation

**Poor (0-4 points):**
- ‚ùå AI is missing or non-functional
- ‚ùå AI is not needed (rule-based would work)
- ‚ùå No innovation
- ‚ùå Poor implementation

---

### 3. Completeness & Polish (15 points)

**What to Evaluate:**
- Are all required deliverables present?
- Is the submission thorough and complete?
- Is there attention to detail and polish?

**Scoring:**

**Excellent (13-15 points):**
- ‚úÖ All deliverables present and high quality
- ‚úÖ Thorough submission with great detail
- ‚úÖ Professional polish evident
- ‚úÖ No major gaps or omissions
- ‚úÖ Presentation is polished

**Good (10-12 points):**
- ‚úÖ All deliverables present
- ‚úÖ Complete submission with good detail
- ‚úÖ Good presentation quality
- Minor areas could be more polished

**Fair (7-9 points):**
- ‚ö†Ô∏è Most deliverables present
- ‚ö†Ô∏è Adequate completeness
- ‚ö†Ô∏è Some areas lack detail
- ‚ö†Ô∏è Presentation could be better

**Poor (4-6 points):**
- ‚ö†Ô∏è Some deliverables missing or minimal
- ‚ö†Ô∏è Incomplete in several areas
- ‚ö†Ô∏è Lacks polish
- ‚ö†Ô∏è Rushed submission evident

**Inadequate (0-3 points):**
- ‚ùå Major deliverables missing
- ‚ùå Very incomplete
- ‚ùå No polish

---

### 4. Impact & Value (15 points)

**What to Evaluate:**
- How much value will this provide?
- Is the impact realistic and achievable?
- Does it solve a real problem?
- Is there business value?

**Scoring:**

**High Impact (13-15 points):**
- ‚úÖ Solves significant, validated problem
- ‚úÖ Clear quantified benefits
- ‚úÖ High number of potential users
- ‚úÖ Strong business value
- ‚úÖ Could become permanent tool
- ‚úÖ ROI is clear and substantial

**Moderate-High Impact (10-12 points):**
- ‚úÖ Solves real problem
- ‚úÖ Some quantified benefits
- ‚úÖ Good potential user base
- ‚úÖ Clear business value
- ‚úÖ Adoption potential
- Positive ROI expected

**Moderate Impact (7-9 points):**
- ‚úÖ Addresses valid problem
- ‚ö†Ô∏è Benefits described but not fully quantified
- ‚ö†Ô∏è Moderate user base
- ‚ö†Ô∏è Some business value
- Limited scalability

**Low Impact (4-6 points):**
- ‚ö†Ô∏è Problem is niche
- ‚ö†Ô∏è Impact is hard to quantify
- ‚ö†Ô∏è Small user base
- ‚ö†Ô∏è Unclear business value
- Questionable ROI

**Minimal Impact (0-3 points):**
- ‚ùå Problem is trivial or theoretical
- ‚ùå No measurable impact
- ‚ùå Very limited or no users
- ‚ùå No business value

---

### 5. Documentation & Presentation (15 points)

**What to Evaluate:**
- Is documentation clear and comprehensive?
- Can someone else use/understand this?
- Is the presentation professional?

**Scoring:**

**Excellent (13-15 points):**
- ‚úÖ Comprehensive, clear documentation
- ‚úÖ Easy to understand and use
- ‚úÖ Professional presentation
- ‚úÖ Great demo video
- ‚úÖ Well-written throughout

**Good (10-12 points):**
- ‚úÖ Good documentation coverage
- ‚úÖ Clear and understandable
- ‚úÖ Good presentation
- ‚úÖ Solid demo video
- Well-executed overall

**Fair (7-9 points):**
- ‚ö†Ô∏è Adequate documentation
- ‚ö†Ô∏è Some areas unclear
- ‚ö†Ô∏è Presentation is acceptable
- ‚ö†Ô∏è Demo video could be better
- Gets the job done

**Poor (4-6 points):**
- ‚ö†Ô∏è Sparse documentation
- ‚ö†Ô∏è Hard to understand
- ‚ö†Ô∏è Poor presentation
- ‚ö†Ô∏è Weak demo video
- Lacking in multiple areas

**Inadequate (0-3 points):**
- ‚ùå Minimal or no documentation
- ‚ùå Very unclear
- ‚ùå Unprofessional
- ‚ùå Poor quality throughout

---

### 6. Innovation & Uniqueness (10 points)

**What to Evaluate:**
- How novel is this solution?
- Is there creative problem-solving?
- What makes it unique or special?

**Scoring:**

**Exceptional (9-10 points):**
- ‚úÖ Highly innovative approach
- ‚úÖ Novel application of AI
- ‚úÖ Creative problem-solving
- ‚úÖ Could inspire future work
- ‚úÖ Strong differentiation

**Strong (7-8 points):**
- ‚úÖ Innovative elements present
- ‚úÖ Creative approach
- ‚úÖ Good differentiation
- Fresh perspective

**Good (5-6 points):**
- ‚úÖ Some innovation
- ‚ö†Ô∏è Solid but not groundbreaking
- ‚ö†Ô∏è Standard approach done well
- Adequate uniqueness

**Fair (3-4 points):**
- ‚ö†Ô∏è Limited innovation
- ‚ö†Ô∏è Conventional approach
- ‚ö†Ô∏è Minimal differentiation
- Generic solution

**Poor (0-2 points):**
- ‚ùå No innovation
- ‚ùå Generic or copied
- ‚ùå No uniqueness

---

## Selection Process

### Step 1: Score All Submissions
- Complete evaluation for all submissions
- Calculate total scores
- Document rationale for each

### Step 2: Rank by Score
- Sort submissions by total score (highest to lowest)
- Identify natural tiers
- Note borderline cases

### Step 3: Apply Shortlisting Criteria

**Automatic Inclusion (Top Performers):**
- Total score ‚â• 80 points ‚Üí Automatic top 10 consideration
- Clear leaders in scoring

**Borderline Evaluation (70-79 points):**
- Review for exceptional strengths
- Consider diversity of problem domains
- Assess presentation readiness for judges

**Lower Tier (<70 points):**
- Generally not shortlisted unless exceptional circumstances
- May highlight for "special recognition" awards

### Step 4: Portfolio Balance
Consider diversity across:
- Problem domains (variety of problems addressed)
- Technical approaches (different AI technologies)
- Team backgrounds (departments, experience levels)
- Impact areas (different business functions)

### Step 5: Finalize Top 10
- Select best 10 submissions for judges
- Prepare summary for each
- Brief judges on highlights and considerations

---

## Shortlisting Decision Framework

### Mandatory Criteria for Top 10:
‚úÖ Score ‚â• 65 points minimum
‚úÖ Working prototype demonstrated
‚úÖ AI integration is functional
‚úÖ Clear value proposition
‚úÖ Professional submission quality

### Tiebreaker Factors:
1. **Innovation level** - more innovative wins
2. **Impact potential** - higher impact wins
3. **Technical excellence** - better execution wins
4. **Presentation quality** - clearer presentation wins
5. **Portfolio diversity** - adds variety to top 10

### Special Considerations:
- **Breakthrough innovation:** May advance despite lower overall score if exceptionally innovative
- **High impact:** Solutions with very high impact potential get extra weight
- **Technical achievement:** Exceptional technical execution deserves recognition
- **Overcoming adversity:** Teams that overcame significant challenges

---

## Feedback for All Teams

### For Top 10 (Advancing to Judges):
```markdown
üéâ **Congratulations! You're in the Top 10!**

Your submission "[Project Title]" has been selected for the final judges panel!

**Your Score: [X]/100 points**

**Scoring Breakdown:**
- Technical Execution: [Score]/25
- AI Integration: [Score]/20
- Completeness: [Score]/15
- Impact & Value: [Score]/15
- Documentation: [Score]/15
- Innovation: [Score]/10

**What Made You Stand Out:**
- [Specific strength 1]
- [Specific strength 2]
- [Specific strength 3]

**Judges Panel Process:**
- Judges will review your submission in detail
- You may be invited for a live demo presentation
- Final winners will be selected from top 10
- Results announced: [Date]

**Prepare for Judges:**
- Ensure demo is working perfectly
- Prepare for potential questions about:
  - [Technical implementation details]
  - [Scaling and production readiness]
  - [Impact quantification]
- Practice your elevator pitch

Congratulations on this achievement! üåü
```

### For Strong Submissions Not in Top 10:
```markdown
**Excellent Work - Strong Submission**

Thank you for your outstanding effort on "[Project Title]"!

**Your Score: [X]/100 points**

Your submission demonstrated strong execution and came very close to the top 10. While not selected for the judges panel this time, your work is commendable.

**Your Strengths:**
- [Strength 1]
- [Strength 2]
- [Strength 3]

**Areas That Could Strengthen Future Submissions:**
- [Constructive feedback 1]
- [Constructive feedback 2]

**Recognition:**
Your team will receive:
- Participation certificates
- Recognition in hackathon showcase
- Potential "Special Recognition" award for [specific strength]

**Moving Forward:**
We encourage you to:
- Continue developing your solution
- Apply learnings to future projects
- Join the AI Innovation Community

Thank you for your creativity and hard work! üôè
```

### For All Other Submissions:
```markdown
**Thank You for Participating**

Thank you for your submission "[Project Title]" and your participation in AI Innovate!

**Your Score: [X]/100 points**

While your submission wasn't selected for the top 10, your participation and effort are valued.

**What We Appreciated:**
- [Positive aspect 1]
- [Positive aspect 2]

**Opportunities for Growth:**
- [Constructive feedback 1]
- [Constructive feedback 2]

**Recognition:**
- Participation certificate
- Access to all winning project showcases
- Invitation to post-hackathon retrospective

**Keep Learning:**
- Join upcoming AI workshops
- Connect with the AI Innovation Community
- Consider future hackathon participation

Thank you for being part of AI Innovate! üöÄ
```

---

## Mentor Coordination

### Calibration Session
Before scoring:
- Review 2-3 sample submissions together
- Discuss scoring interpretation
- Align on standards and expectations
- Practice using rubrics

### During Evaluation
- Communicate on borderline cases
- Share technical insights
- Discuss controversial decisions
- Maintain consistency

### Final Selection Meeting
- Present all scores
- Discuss top candidates
- Resolve any disputes
- Finalize top 10 list
- Prepare judges briefing

---

## Judging Panel Briefing

### Information Package for Judges:
For each top 10 submission, provide:
- Executive summary (1 page)
- Total score and breakdown
- Key strengths and highlights
- Areas to probe during review
- Demo video link
- Live demo access (if available)
- GitHub repository link

### Briefing Agenda:
1. Hackathon overview and objectives
2. Evaluation process summary
3. Top 10 submissions overview
4. Judging criteria for final selection
5. Schedule and format
6. Q&A

---

## Success Metrics

**Process Success:**
- All submissions reviewed within timeline
- Consistent scoring across mentors
- Clear rationale for top 10 selection
- High-quality feedback provided to all teams

**Outcome Success:**
- Top 10 represent best submissions objectively
- Diversity of approaches and problems
- Strong candidates for final judging
- Participant satisfaction with fairness

**Continuous Improvement:**
- Document evaluation challenges
- Note any rubric ambiguities
- Gather mentor feedback
- Identify process improvements for next year
