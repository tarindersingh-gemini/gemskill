# Stage 3: Proposal Evaluation Criteria

## Overview
This document defines the comprehensive evaluation criteria for Stage 3 (Proposal Review and Approval) of the AI Innovate Hackathon. These criteria are used at two levels:
- **Level 1 (Automated):** GitHub Copilot agent for initial filtering
- **Level 2 (Manual):** Mentor review for final selection

## Evaluation Philosophy
We seek proposals that demonstrate:
- **Innovation:** Novel applications of AI technology
- **Feasibility:** Realistic scope and resource requirements
- **Business Value:** Clear benefits to Gemini and its employees
- **Team Capability:** Strong likelihood of successful execution

---

## Scoring Framework

### Total Score: 100 Points

| **Criterion** | **Weight** | **Points** | **Evaluator** |
|---------------|------------|------------|---------------|
| Innovation & Creativity | 25% | 25 | Both |
| Technical Feasibility | 20% | 20 | Both |
| Business Value & Impact | 20% | 20 | Both |
| Resource Reasonableness | 15% | 15 | Both |
| Team Capability | 10% | 10 | Manual only |
| Clarity & Completeness | 10% | 10 | Automated only |

---

## Detailed Evaluation Criteria

### 1. Innovation & Creativity (25 points)

**What We're Evaluating:**
How novel, creative, and differentiated is this AI solution?

**Scoring Rubric:**

**Exceptional (21-25 points):**
- Novel application of AI not seen before in the organization
- Unique combination of AI technologies or approaches
- Could set new standards or inspire future innovations
- Demonstrates deep creative thinking
- Clear differentiation from existing solutions

**Strong (16-20 points):**
- Fresh take on a known problem
- Applies AI in a non-obvious way
- Some unique elements combined with proven approaches
- Shows creative problem-solving
- Meaningfully different from existing solutions

**Good (11-15 points):**
- Solid application of AI to a real problem
- Some innovative elements
- Builds on existing approaches with improvements
- Demonstrates understanding of AI capabilities
- Clear but not groundbreaking differentiation

**Fair (6-10 points):**
- Standard or conventional AI application
- Minimal innovative elements
- Similar to existing solutions with minor variations
- Limited creative thinking demonstrated
- Questionable differentiation

**Poor (0-5 points):**
- Generic or basic AI application
- No innovative elements
- Direct copy of existing solutions
- Problem doesn't benefit from AI (rule-based solution would work)
- No clear differentiation

**Key Questions:**
- Is this a novel use of AI technology?
- Does it solve the problem in a new or unexpected way?
- How different is this from existing solutions?
- Could this inspire future innovations?

---

### 2. Technical Feasibility (20 points)

**What We're Evaluating:**
Can this be realistically built in 3 weeks with available resources?

**Scoring Rubric:**

**Highly Feasible (17-20 points):**
- Scope is appropriate for 3-week timeline
- Required technologies are accessible and well-documented
- Team has relevant technical experience
- Clear, logical implementation plan
- Minimal dependencies on unavailable resources
- Technical approach is sound and proven
- Risk factors are identified and manageable

**Feasible (13-16 points):**
- Scope is achievable but ambitious for timeline
- Technologies are accessible with some learning curve
- Implementation plan is reasonable
- Some dependencies but mostly manageable
- Technical approach is solid
- Moderate risks with mitigation strategies

**Moderately Feasible (9-12 points):**
- Scope is challenging for 3-week timeline
- Some technologies may require significant learning
- Implementation plan has gaps
- Several external dependencies
- Technical approach has some uncertain elements
- Higher risk factors present

**Questionable Feasibility (5-8 points):**
- Scope is very ambitious for timeline
- Requires specialized/unavailable technologies
- Implementation plan is vague or incomplete
- Many external dependencies or blockers
- Technical approach has significant uncertainties
- High-risk factors with no mitigation

**Not Feasible (0-4 points):**
- Scope is unrealistic for 3-week timeline
- Requires inaccessible or proprietary technologies
- No clear implementation plan
- Critical dependencies cannot be met
- Technical approach is flawed or impossible
- Insurmountable risk factors

**Key Questions:**
- Can this be built in 3 weeks?
- Are the required technologies accessible?
- Is the technical approach sound?
- Are there any blocking dependencies?
- What are the main risks and are they manageable?

---

### 3. Business Value & Impact (20 points)

**What We're Evaluating:**
How much value will this solution provide to Gemini and its employees?

**Scoring Rubric:**

**High Impact (17-20 points):**
- Solves a significant, widespread problem
- Benefits many employees (50+ people) or entire departments
- Quantifiable impact (time saved, cost reduced, quality improved)
- Could become a permanent organizational tool
- Aligns with company strategic goals
- Has potential for broader adoption or scaling
- ROI is clear and substantial

**Moderate-High Impact (13-16 points):**
- Solves a real problem affecting multiple teams
- Benefits 20-50 employees or specific departments
- Some quantifiable benefits
- Could be adopted beyond the hackathon
- Supports organizational objectives
- Scaling potential exists
- Positive ROI expected

**Moderate Impact (9-12 points):**
- Solves a valid problem for a team or department
- Benefits 10-20 employees
- Impact is described but not fully quantified
- May have adoption potential
- Loosely aligned with company goals
- Limited scaling potential
- ROI is unclear but possible

**Low Impact (5-8 points):**
- Solves a niche problem
- Benefits fewer than 10 employees
- Impact is difficult to quantify
- Limited adoption potential
- Weak alignment with company goals
- No clear scaling path
- ROI is questionable

**Minimal Impact (0-4 points):**
- Problem is trivial or hypothetical
- Benefits very few or no employees
- No measurable impact
- No adoption potential
- No alignment with company goals
- Cannot scale
- No ROI

**Key Questions:**
- How many people will benefit?
- What is the quantifiable impact?
- Could this become a permanent tool?
- Does it align with organizational goals?
- What is the potential ROI?

---

### 4. Resource Reasonableness (15 points)

**What We're Evaluating:**
Are the requested resources reasonable, justified, and available?

**Scoring Rubric:**

**Very Reasonable (13-15 points):**
- Minimal or zero cost using free tiers
- All resources are readily available
- No special hardware requirements
- Uses standard development tools
- Resource requests are well-justified
- Budget is detailed and realistic
- No special permissions or access needed

**Reasonable (10-12 points):**
- Low cost (<$100) with clear justification
- Most resources are available
- Standard computing resources sufficient
- Common development tools and APIs
- Resource requests are justified
- Budget is reasonable
- May need some standard permissions

**Moderately Reasonable (7-9 points):**
- Moderate cost ($100-$300) with justification
- Some resources need procurement
- May need enhanced computing resources
- Some specialized tools or paid APIs
- Resource requests are somewhat justified
- Budget could be optimized
- Requires special permissions or approvals

**Concerning (4-6 points):**
- High cost ($300-$1000) with weak justification
- Many resources need procurement
- Requires specialized hardware
- Multiple paid services needed
- Resource requests are poorly justified
- Budget is inflated or unclear
- Requires multiple approvals

**Unreasonable (0-3 points):**
- Excessive cost (>$1000)
- Resources are unavailable or restricted
- Requires expensive specialized hardware
- Needs proprietary or inaccessible APIs
- Resource requests are unjustified
- Budget is unrealistic
- Cannot obtain necessary permissions

**Key Questions:**
- What is the total estimated cost?
- Are all requested resources available?
- Are the resource requests justified?
- Are there any blocking resource requirements?
- Could resources be optimized?

---

### 5. Team Capability (10 points) - Manual Review Only

**What We're Evaluating:**
Does the team have the skills, experience, and commitment to execute successfully?

**Scoring Rubric:**

**Strong Team (9-10 points):**
- Team has relevant AI/ML experience
- Complementary skill sets (dev, design, domain knowledge)
- Clear role distribution
- Previous collaboration or project experience
- Strong commitment and availability
- Track record of completing projects
- Diverse perspectives and expertise

**Capable Team (7-8 points):**
- Some AI/ML experience in team
- Good skill coverage
- Reasonable role distribution
- Team members are available
- Demonstrated commitment
- Some project experience
- Adequate expertise for the project

**Developing Team (5-6 points):**
- Limited AI/ML experience
- Some skill gaps
- Role distribution unclear
- Availability concerns
- Uncertain commitment level
- Limited project experience
- May need significant mentoring

**Weak Team (3-4 points):**
- Minimal relevant experience
- Significant skill gaps
- Poor role distribution or solo effort
- Availability issues
- Low commitment indicators
- No project experience
- Heavy mentoring needed

**Inadequate Team (0-2 points):**
- No relevant experience
- Critical skill gaps
- No clear team structure
- Major availability issues
- Unclear commitment
- Cannot execute project scope
- Not ready for hackathon

**Key Questions:**
- Does the team have AI/ML experience?
- Are the skill sets complementary?
- Are roles clearly defined?
- Is the team available and committed?
- Can they realistically execute this project?

---

### 6. Clarity & Completeness (10 points) - Automated Review Only

**What We're Evaluating:**
Is the proposal clear, complete, and well-structured?

**Scoring Rubric:**

**Excellent (9-10 points):**
- All sections thoroughly completed
- Problem and solution are crystal clear
- Technical approach is well-explained
- Resources are specifically identified
- Deliverables are clearly defined
- No ambiguity or vagueness
- Professional presentation

**Good (7-8 points):**
- All sections completed
- Problem and solution are clear
- Technical approach is understandable
- Resources are identified
- Deliverables are defined
- Minor areas of ambiguity
- Good presentation

**Fair (5-6 points):**
- Most sections completed
- Problem or solution has some clarity issues
- Technical approach needs more detail
- Resources are somewhat vague
- Deliverables are partially defined
- Some ambiguity present
- Adequate presentation

**Poor (3-4 points):**
- Several sections incomplete
- Problem or solution is unclear
- Technical approach is vague
- Resources are not well-defined
- Deliverables are unclear
- Significant ambiguity
- Poor presentation

**Inadequate (0-2 points):**
- Many sections incomplete
- Problem and solution are very unclear
- No clear technical approach
- Resources undefined
- Deliverables not specified
- Highly ambiguous
- Unprofessional presentation

---

## Decision Thresholds

### Automated Level 1 Filtering
**Purpose:** Filter out proposals that clearly don't meet minimum standards

**Auto-Reject if:**
- Total automated score < 30 points (Innovation + Feasibility + Business Value + Resource + Clarity)
- Any single criterion scores 0-4 points
- Critical red flags (see Copilot agent instructions)

**Forward to Manual Review if:**
- Total automated score â‰¥ 30 points
- No criterion scores below 5 points
- No critical red flags

### Manual Level 2 Selection
**Purpose:** Select the most promising proposals for approval

**Approval Threshold:**
- Total score (including Team Capability) â‰¥ 60 points
- No criterion scores below 6 points (except Clarity which is automated)
- At least one criterion scores 17+ points (strength area)

**Selection Process:**
1. Score each proposal using the full rubric
2. Rank proposals by total score
3. Consider team diversity and problem domain coverage
4. Select proposals until resource capacity is reached
5. Provide feedback to all teams (approved and declined)

---

## Mentor Guidance

### Evaluation Best Practices

**1. Read Holistically First**
- Read the entire proposal before scoring
- Get a general sense of the idea and team
- Note initial impressions and concerns

**2. Score Each Criterion Independently**
- Evaluate each criterion against its specific rubric
- Don't let one strong/weak area bias others
- Be objective and consistent across proposals

**3. Consider Context**
- Participant experience level
- Problem domain complexity
- Available resources and time constraints

**4. Be Fair and Constructive**
- Look for potential, not just polish
- Consider how mentoring could strengthen weak areas
- Balance innovation vs. execution capability

**5. Document Rationale**
- Write brief notes explaining scores
- Identify specific strengths and concerns
- Provide actionable feedback

### Common Evaluation Pitfalls to Avoid

âŒ **Over-valuing novelty** - Innovation is important, but not at the expense of feasibility
âŒ **Under-valuing impact** - Unsexy but high-impact solutions deserve consideration
âŒ **Perfection bias** - Proposals don't need to be perfect, just viable
âŒ **Complexity bias** - Simple, well-executed ideas can be better than complex ambitious ones
âŒ **Familiarity bias** - Don't favor ideas in your domain of expertise
âŒ **Team reputation bias** - Evaluate the proposal, not who submitted it

### Calibration Exercise

Before evaluating submissions, mentors should:
1. Review sample proposals together
2. Score them independently
3. Compare scores and discuss differences
4. Align on interpretation of rubrics
5. Establish consistent standards

---

## Approval Capacity Planning

### Resource Considerations
- Mentor availability for guidance during development (Week 4-6)
- Technical resource availability (APIs, computing, tools)
- Maximum number of teams that can be effectively supported

### Recommended Approval Rates
- Target: Approve 40-60% of valid submissions
- For 27-55 expected teams: Aim to approve 15-30 teams
- Balance: Innovation/risk vs. feasibility/success rate

### Waitlist Strategy
- Create ranked waitlist for borderline proposals (scores 55-59)
- If approved teams drop out, invite waitlist teams
- Provide feedback to waitlist teams on what would strengthen their proposal

---

## Feedback Templates

### Approval Notification
```markdown
ðŸŽ‰ **Congratulations! Your proposal has been approved!**

Your team's proposal "[Problem Title]" has been selected to move forward in the AI Innovate Hackathon!

**Evaluation Summary:**
- Innovation & Creativity: [Score]/25
- Technical Feasibility: [Score]/20
- Business Value & Impact: [Score]/20
- Resource Reasonableness: [Score]/15
- Team Capability: [Score]/10
- **Total Score: [Score]/90**

**Strengths:**
- [Specific strength 1]
- [Specific strength 2]

**Areas for Focus:**
- [Constructive suggestion 1]
- [Constructive suggestion 2]

**Next Steps:**
1. Your proposal has been moved to the `approved` directory
2. Resource allocation team will contact you within 2 business days
3. Development officially starts [Start Date]
4. Midway check-in is due [Midway Date]
5. Join the team channel: #hackathon-team-[team-name]

**Assigned Mentor:** [Mentor Name] - [Email]

Best of luck with development! We're excited to see what you build! ðŸš€
```

### Decline Notification
```markdown
**Thank you for your submission**

Thank you for submitting your proposal "[Problem Title]" to the AI Innovate Hackathon. After careful evaluation by our mentor team, we have decided not to move forward with this proposal at this time.

**Evaluation Summary:**
- Innovation & Creativity: [Score]/25
- Technical Feasibility: [Score]/20
- Business Value & Impact: [Score]/20
- Resource Reasonableness: [Score]/15
- Team Capability: [Score]/10
- **Total Score: [Score]/90**

**What We Appreciated:**
- [Positive aspect 1]
- [Positive aspect 2]

**Areas That Limited Selection:**
- [Constructive feedback 1]
- [Constructive feedback 2]

**Recommendations:**
- [Suggestion for improvement or alternative approach]
- [Encouragement to participate in future hackathons]

We received many strong proposals and had to make difficult choices based on resource constraints and strategic fit. We encourage you to:
- Apply these learnings to future hackathon submissions
- Continue exploring AI technologies in your daily work
- Join our AI Innovation Community for ongoing learning

Thank you for your participation and creativity! ðŸ™
```

### Waitlist Notification
```markdown
**Your proposal is on the waitlist**

Thank you for submitting your proposal "[Problem Title]" to the AI Innovate Hackathon. Your idea showed promise, and we've placed it on our ranked waitlist.

**Evaluation Summary:**
- Innovation & Creativity: [Score]/25
- Technical Feasibility: [Score]/20
- Business Value & Impact: [Score]/20
- Resource Reasonableness: [Score]/15
- Team Capability: [Score]/10
- **Total Score: [Score]/90**

**What This Means:**
- If any approved teams withdraw, we'll invite waitlist teams in rank order
- We'll notify you within [Date] if a spot opens up
- Your proposal scored well but we hit resource capacity limits

**Strengths:**
- [Specific strength 1]
- [Specific strength 2]

**To Strengthen for Future:**
- [Constructive suggestion 1]
- [Constructive suggestion 2]

**Stay Ready:**
- Keep your team available and engaged
- Continue refining your approach
- Check email for potential approval updates

Thank you for your excellent submission! ðŸŒŸ
```

---

## Continuous Improvement

### Post-Stage 3 Review
After proposal review is complete, mentors should:
1. Review scoring distribution and consistency
2. Identify any evaluation challenges or ambiguities
3. Refine criteria for next hackathon
4. Document lessons learned
5. Gather feedback on evaluation process

### Success Metrics
- Approval rate aligned with targets (40-60%)
- Approved teams complete development (>80%)
- High quality final submissions from approved teams
- Participants report clear, fair evaluation process
- Mentors report efficient, consistent evaluation
